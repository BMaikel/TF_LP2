{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa9ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas  as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c32338",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://repositorio.lamolina.edu.pe/recent-submissions?offset=\"\n",
    "\n",
    "# Realizar una petición HTTP a la URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Analizar el contenido HTML de la página usando Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Crea el limite superior del for tomando en cuenta la cantidad de tesis expuestas en  la pagina\n",
    "limsup=int((soup.find(\"p\",{\"class\":\"pagination-info\"})).text[-4:])/20+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d37063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs=[]\n",
    "# Iterar sobre todos los valores de i desde 0 hasta 100\n",
    "for i in range(0, int(limsup)):\n",
    "    # Crear la URL de la página a scrappear\n",
    "    url = \"https://repositorio.lamolina.edu.pe/recent-submissions?offset=\" + str(i*20)\n",
    "\n",
    "    # Realizar una petición HTTP a la URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Analizar el contenido HTML de la página usando Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extraer todos los enlaces de la página\n",
    "    links = soup.find_all(\"h4\",{\"class\":\"artifact-title\"})\n",
    "\n",
    "\n",
    "    # Iterar sobre todos los enlaces extraídos y mostrarlos por pantalla\n",
    "    for link in links:\n",
    "        hrefs.append(\"https://repositorio.lamolina.edu.pe\"+link.find(\"a\").get(\"href\")+\"?show=full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3fa863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "institucion=[]\n",
    "titulo=[]\n",
    "autor=[]\n",
    "grado=[]\n",
    "asesor=[]\n",
    "resumen=[]\n",
    "anio=[]\n",
    "vacio=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b597fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(hrefs)):\n",
    "    response=requests.get(hrefs[i])\n",
    "    \n",
    "    # Analizar el contenido HTML de la página usando Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extraer la tabla que deseas\n",
    "    table = soup.find(\"table\", {\"class\": \"ds-includeSet-table detailtable table table-striped table-hover\"})\n",
    "\n",
    "    # Convertir la tabla en un DataFrame de Pandas\n",
    "    df = pd.read_html(str(table),index_col=0)[0]\n",
    "    \n",
    "    # Nos quedamos con la parte de la tabla que nos importa\n",
    "    df=df[:][1]\n",
    "    \n",
    "    # Scrappeo de datos\n",
    "    if \"dc.publisher\" in df:\n",
    "        institucion.append(df[\"dc.publisher\"])\n",
    "    else:\n",
    "        institucion.append(vacio)\n",
    "    if \"dc.title\" in  df:\n",
    "        titulo.append(df[\"dc.title\"])\n",
    "    else:\n",
    "        titulo.append(vacio)\n",
    "    \n",
    "    if \"dc.contributor.author\" in df:\n",
    "        if type(df[\"dc.contributor.author\"])==object :\n",
    "            autor.append(df[\"dc.contributor.author\"][:].apply(lambda x: x).tolist())\n",
    "        else :\n",
    "            autor.append(df[\"dc.contributor.author\"])\n",
    "    else:\n",
    "        autor.append(vacio)\n",
    "    \n",
    "    if \"dc.type\" in df:\n",
    "        grado.append(df[\"dc.type\"])\n",
    "    else:\n",
    "        grado.append(vacio)\n",
    "        \n",
    "    if \"dc.contributor.advisor\" in df:\n",
    "        if type(df[\"dc.contributor.advisor\"])==object :\n",
    "            asesor.append(df[\"dc.contributor.advisor\"].apply(lambda x: x).tolist())\n",
    "        else :\n",
    "            asesor.append(df[\"dc.contributor.advisor\"])\n",
    "    else:\n",
    "        asesor.append(vacio)\n",
    "    \n",
    "    if \"dc.description.abstract\" in df:\n",
    "        resumen.append(df[\"dc.description.abstract\"][0])\n",
    "    else:\n",
    "        resumen.append(vacio)\n",
    "    \n",
    "    if \"dc.date.issued\" in df:\n",
    "        anio.append(df[\"dc.date.issued\"])\n",
    "    else:\n",
    "        anio.append(vacio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cac9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datosexportar = pd.DataFrame(list(zip(anio,institucion,titulo,autor,grado,asesor,resumen)),columns =[\"Año\",\"Institución\",\"Titulo\",\"Autor\",\"Grado\",\"Asesor\",\"Resumen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c2f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "datosexportar.to_csv(\"repositorio_agraria_tesis.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f73e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
